{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# to break into sentences from text\n",
    "def sentence(text):\n",
    "    sentences = []\n",
    "    sentences = list(text.split(\"\\n\"))\n",
    "    return sentences\n",
    "    \n",
    "\n",
    "# load the document\n",
    "filename = 'a1_data/a1_d3.txt'\n",
    "text = load_doc(filename)\n",
    "sentences = sentence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing\n",
    "lower_case_sentences = []\n",
    "for i in sentences:\n",
    "    lower_case_sentences.append(i.lower())\n",
    "\n",
    "no_punctuations = []\n",
    "for i in lower_case_sentences:\n",
    "    no_punctuations.append(''.join(c for c in i if c not in string.punctuation))\n",
    "\n",
    "clean_data = []\n",
    "for i in no_punctuations:\n",
    "    sub = i.split(', ')\n",
    "    sub1 = sub[0].split('\\t')\n",
    "    clean_data.append(sub1)\n",
    "clean_data.remove(clean_data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review Sentiment\n",
      "0    so there is no way for me to plug it in here i...         0\n",
      "1                            good case excellent value         1\n",
      "2                                great for the jawbone         1\n",
      "3    tied to charger for conversations lasting more...         0\n",
      "4                                     the mic is great         1\n",
      "..                                                 ...       ...\n",
      "995  the screen does get smudged easily because it ...         0\n",
      "996  what a piece of junk i lose more calls on this...         0\n",
      "997                        item does not match picture         0\n",
      "998  the only thing that disappoint me is the infra...         0\n",
      "999  you can not answer calls with the unit never w...         0\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(clean_data, columns =['Review', 'Sentiment'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into 5 folds\n",
    "def cross_validation(df, n_folds):\n",
    "    df_split = list()\n",
    "    df_copy = list(df)\n",
    "    fold_size = int(len(df) / 5)\n",
    "    for i in range (n_folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            index = random.randrange(0,len(df_copy))\n",
    "            fold.append(df_copy.pop(index))\n",
    "        df_split.append(fold)\n",
    "    return df_split\n",
    "\n",
    "folds = cross_validation(clean_data, 5)\n",
    "for fold in folds:\n",
    "    train_set = list(folds)\n",
    "    train_set.remove(fold)\n",
    "    train_set = sum(train_set, [])\n",
    "    test_set = list()\n",
    "    for row in fold:\n",
    "        row_copy = list(row)\n",
    "        test_set.append(row_copy)\n",
    "        \n",
    "train_df = pd.DataFrame(train_set, columns =['Review', 'Sentiment'])\n",
    "test_df = pd.DataFrame(test_set,columns =['Review', 'Sentiment'])\n",
    "train_df_positive = train_df.loc[train_df['Sentiment']=='1']\n",
    "train_df_negative = train_df.loc[train_df['Sentiment']=='0']\n",
    "\n",
    "# Setting the model's vocabulary\n",
    "def vocab_freq(train_df):\n",
    "    train_sentences = train_df['Review'].values\n",
    "    train_sentences_list = train_sentences.tolist()\n",
    "    all_words_train = []\n",
    "    for i in train_sentences_list:\n",
    "        all_words_train.extend(i.split(' '))\n",
    "    vocab,count = np.unique(np.array(all_words_train),return_counts=True)\n",
    "    return (vocab,count)\n",
    "\n",
    "#Setting the positive sentiment and negative sentiment vocab and frequency\n",
    "vocab_positive, count_positive = vocab_freq(train_df_positive)\n",
    "vocab_negative, count_negative = vocab_freq(train_df_negative)\n",
    "vocab_total, count_total = vocab_freq(train_df)\n",
    "\n",
    "\n",
    "#Gives the probability P(C) or prior probability\n",
    "# no. of sentiment values is the same as the no. of reviews in train_set\n",
    "train_sentiments = train_df['Sentiment'].values\n",
    "sentiment,count = np.unique(train_sentiments,return_counts=True)\n",
    "\n",
    "positive_review_count = count[1]\n",
    "negative_review_count = count[0]\n",
    "\n",
    "prob_positive = positive_review_count / (positive_review_count + negative_review_count)\n",
    "prob_negative = negative_review_count / (positive_review_count + negative_review_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extracting the words from the test_set\n",
    "test_sentences = test_df['Review'].values\n",
    "test_sentences_list = test_sentences.tolist()\n",
    "words_test = []\n",
    "for i in test_sentences_list:\n",
    "    words_test.append(i.split(' '))\n",
    "\n",
    "#Calculating likelihood probability P(d|C)\n",
    "#Writing a function for a given class C = 1,0\n",
    "def posterior_prob(train_df,vocab,count,words_test,prob_class, class_count):\n",
    "    posterior_prob = list()\n",
    "    #Calculations for test data in row i\n",
    "    for i in words_test:\n",
    "        likelihood_prob = 1\n",
    "        word_test_array = np.array(i)\n",
    "        vocab_test,count_test = np.unique(word_test_array,return_counts=True)\n",
    "        #j returns the elements of the iterable list i\n",
    "        for j in i:\n",
    "            try:\n",
    "                index = list(vocab).index(j)\n",
    "                #here likelihood probability is returned for the ith row of test data\n",
    "                likelihood_prob *= ((count[index]+ 1)/(np.sum(count)+np.sum(count_total)+1))\n",
    "            except ValueError:\n",
    "                likelihood_prob *= ((0+ 1)/(np.sum(count)+np.sum(count_total)+1))\n",
    "            \n",
    "        #return the probability P(d|C)*P(C)\n",
    "        posterior = prob_class*likelihood_prob\n",
    "        posterior_prob.append(posterior)\n",
    "    return posterior_prob\n",
    "            \n",
    "posterior_prob_positive = posterior_prob(train_df,vocab_positive,count_positive,words_test,prob_positive,positive_review_count)\n",
    "posterior_prob_negative = posterior_prob(train_df,vocab_negative,count_negative,words_test,prob_negative,negative_review_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing f1 score\n",
    "def f1_score_single(y_actual, y_pred):\n",
    "    y_actual = set(y_actual)\n",
    "    y_pred = set(y_pred)\n",
    "    true_positives = len(y_actual & y_pred)\n",
    "    if true_positives == 0: return 0.\n",
    "    precision = 1.0* true_positives / len(y_pred)\n",
    "    recall = 1.0* true_positives / len(y_actual)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "def f1_score(y_actual, y_pred):\n",
    "    return np.mean([f1_score_single(x, y) for x, y in zip(y_actual, y_pred)])\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review Sentiment  \\\n",
      "0                                         fast service         1   \n",
      "1    sound quality on both end is excellent i use h...         1   \n",
      "2    this phone is slim and light and the display i...         1   \n",
      "3                   plantronics bluetooth excelent buy         1   \n",
      "4                                          great phone         1   \n",
      "..                                                 ...       ...   \n",
      "195  the design might be ergonomic in theory but i ...         0   \n",
      "196  this phone is pretty sturdy and ive never had ...         1   \n",
      "197  excellent product i am very satisfied with the...         1   \n",
      "198         yes its shiny on front side  and i love it         1   \n",
      "199        i wasted my little money with this earpiece         0   \n",
      "\n",
      "    Predicted Sentiment  \n",
      "0                     1  \n",
      "1                     1  \n",
      "2                     1  \n",
      "3                     1  \n",
      "4                     1  \n",
      "..                  ...  \n",
      "195                   0  \n",
      "196                   1  \n",
      "197                   1  \n",
      "198                   1  \n",
      "199                   0  \n",
      "\n",
      "[200 rows x 3 columns]\n",
      "0.815\n",
      "81.5\n"
     ]
    }
   ],
   "source": [
    "test_predict = list()\n",
    "\n",
    "#predict the Sentiment\n",
    "for i in range (len(test_set)):\n",
    "    if posterior_prob_positive[i] > posterior_prob_negative[i]:\n",
    "        test_predict.append(\"1\")\n",
    "    else:\n",
    "        test_predict.append(\"0\")\n",
    "        \n",
    "test_df['Predicted Sentiment'] = test_predict \n",
    "f1_score = f1_score(test_df['Sentiment'], test_df['Predicted Sentiment'])\n",
    "accuracy = accuracy_metric(test_df['Sentiment'], test_df['Predicted Sentiment'])\n",
    "print(test_df)\n",
    "print(f1_score)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
