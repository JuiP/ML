{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : \n",
      "\n",
      "[[8.450e+03 7.000e+00 5.000e+00 ... 0.000e+00 5.480e+02 1.000e+00]\n",
      " [9.600e+03 6.000e+00 8.000e+00 ... 1.000e+00 4.600e+02 1.000e+00]\n",
      " [1.125e+04 7.000e+00 5.000e+00 ... 1.000e+00 6.080e+02 1.000e+00]\n",
      " ...\n",
      " [9.042e+03 7.000e+00 9.000e+00 ... 2.000e+00 2.520e+02 1.000e+00]\n",
      " [9.717e+03 5.000e+00 6.000e+00 ... 0.000e+00 2.400e+02 0.000e+00]\n",
      " [9.937e+03 5.000e+00 6.000e+00 ... 0.000e+00 2.760e+02 0.000e+00]]\n",
      "\n",
      "Dimensions of dataset : (1460, 11)\n"
     ]
    }
   ],
   "source": [
    "data_orig = np.genfromtxt('data/housepricedata.csv',delimiter=',',skip_header=1)\n",
    "print(\"Dataset : \\n\\n\"+ str(data_orig))\n",
    "print(\"\\nDimensions of dataset : \"+str(data_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Y   :[1. 1. 1. ... 1. 0. 0.]\n",
      "Shape of Y : (1460,)\n"
     ]
    }
   ],
   "source": [
    "#Extacting Y\n",
    "y_orig = data_orig[:,-1]\n",
    "print(\"Output Y   :\"+str(y_orig))\n",
    "print(\"Shape of Y : \"+str(y_orig.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (1, 1460)\n",
      "49.86301369863014\n"
     ]
    }
   ],
   "source": [
    "#Removing Rank 1 array\n",
    "Y = np.reshape(y_orig,(y_orig.shape[0],1)).T    \n",
    "print(\"Shape of Y: \"+ str(Y.shape))\n",
    "print((np.sum(Y)/1460)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1460)\n"
     ]
    }
   ],
   "source": [
    "#Extracting vectorized input feature X (transposed)\n",
    "X = data_orig[:,0:-1].T\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    x_mean = np.mean(x,axis=1, keepdims=True)\n",
    "    x_std = np.std(x, axis=1, keepdims=True)+0.0000001\n",
    "\n",
    "    X = (x - x_mean)/x_std   #Python Broadcasting\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of Training set X : (10, 1168)\n",
      "Shape of Training set Y : (1, 1168)\n",
      "\n",
      "Shape of Test set   X   : (10, 292)\n",
      "Shape of Test set Y     : (1, 292)\n"
     ]
    }
   ],
   "source": [
    "#Splitting into Train, Test sets ( with a fixed seed )\n",
    "train_split_percent = 80\n",
    "test_split_percent = 20\n",
    "\n",
    "train_X , test_X = X[:, : int( (train_split_percent/100)*X.shape[1])] , X[:,int( (train_split_percent/100)*X.shape[1]) : ]\n",
    "train_Y , test_Y = Y[:, : int( (train_split_percent/100)*X.shape[1])] , Y[:,int( (train_split_percent/100)*X.shape[1]) : ]\n",
    "print(\"\\nShape of Training set X : \"+str(train_X.shape))\n",
    "print(\"Shape of Training set Y : \"+str(train_Y.shape))\n",
    "print(\"\\nShape of Test set   X   : \"+str(test_X.shape))\n",
    "print(\"Shape of Test set Y     : \"+str(test_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training examples : 1168\n",
      "No of test example      : 292\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "m_train = train_X.shape[1]\n",
    "m_test  = test_X.shape[1]\n",
    "print(\"No of training examples : \"+str(m_train))\n",
    "print(\"No of test example      : \"+str(m_test))\n",
    "print((np.sum(1-test_Y)/292)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    sigz= 1/(1+np.exp(-Z))\n",
    "    sigz[sigz==1] = 0.99999999999             #Incase of flow\n",
    "    sigz[sigz==0] = 0.000000000001\n",
    "    return sigz        \n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x)*0.1\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.1\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    p = {\"W1\": W1,\"b1\": b1,   \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "    return p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            \n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*(2/layer_dims[l-1])**0.5\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "   \n",
    "    Z = np.dot(W,A)+b\n",
    "    #Z = standardize(Z) Batch-Normalize with u,var=1\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation,layer):\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z), sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z), relu(Z)\n",
    "        dropout_cache = A\n",
    "        D = np.random.rand(A.shape[0],A.shape[1]) \n",
    "        if layer==1:\n",
    "            D[:,:]=1\n",
    "        else:\n",
    "            D = (D < keep_prob).astype(int)                                         \n",
    "            A = A*D                                         \n",
    "            A = A/keep_prob \n",
    "        global Dcache \n",
    "        Dcache = D\n",
    "    \n",
    "    cache = (linear_cache, activation_cache,Dcache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(X, parameters):\n",
    "\n",
    "    caches = []\n",
    "    D = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)],\"relu\",l)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)],\"sigmoid\",l)\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y,parameters):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    #print(AL)\n",
    "    cost = (-1/m)*(np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL)))\n",
    "    sumW = 0\n",
    "    L = len(parameters) // 2 \n",
    "    for l in range(1, L):\n",
    "        sumW= sumW + np.sum(np.square(parameters[\"W\"+str(l)]))\n",
    "        \n",
    "    L2_cost= lambd*(sumW)/(2*m)\n",
    "    cost = cost + L2_cost\n",
    "    cost = np.squeeze(cost)     \n",
    "   \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, linear_cache,keep_prob):\n",
    "    \n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation,keep_prob):\n",
    "\n",
    "    linear_cache, activation_cache, dropout_cache = cache\n",
    "    global dA_prev, dW, db\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache,keep_prob)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache,keep_prob=1)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardprop(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    #print(caches[-2][-1].shape)\n",
    "    #print(L)\n",
    "    \n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache,\"sigmoid\",keep_prob=1)\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        global Dprev_cache\n",
    "        D_prev = caches[l-1][2]\n",
    "        global dA_prev_temp, dW_temp, db_temp\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache,\n",
    "                                                                \"relu\",keep_prob)\n",
    "        if l > 0:\n",
    "            dA_prev_temp = np.multiply(dA_prev_temp,D_prev)\n",
    "            dA_prev_temp = dA_prev_temp/keep_prob\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "\n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1]))\n",
    "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1]))\n",
    "   \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adam_optimizer(parameters, grads, v, s, t,m, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8,):\n",
    "\n",
    "    L = len(parameters) // 2                 \n",
    "    v_corrected = {}                         \n",
    "    s_corrected = {}                        \n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)]+(1-beta1)*grads['dW'+str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)]+(1-beta1)*grads['db'+str(l+1)]\n",
    "       \n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-pow(beta1,t)) \n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-pow(beta1,t))\n",
    "        \n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)]+(1-beta2)*np.power(grads['dW'+str(l+1)],2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)]+(1-beta2)*np.power(grads['db'+str(l+1)],2)\n",
    "\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-pow(beta2,t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-pow(beta2,t))\n",
    "\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*np.divide(v_corrected[\"dW\" + str(l+1)],np.sqrt(s_corrected[\"dW\" + str(l+1)])+epsilon)\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\"+ str(l+1)] +(lambd/m)*parameters[\"W\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*np.divide(v_corrected[\"db\" + str(l+1)],np.sqrt(s_corrected[\"db\" + str(l+1)])+epsilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate0 = 0.003, epocs = 3000,\n",
    "                  beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, print_after=1):\n",
    "\n",
    "    costs = []                      \n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    v, s = initialize_adam(parameters)\n",
    "    \n",
    "    t = 0\n",
    "    m=X.shape[1]\n",
    "    \n",
    "    for i in range(0, epocs):\n",
    "        AL, caches = forwardprop(X, parameters)\n",
    "        cost = compute_cost(AL, Y,parameters)\n",
    "        grads = backwardprop(AL, Y, caches)\n",
    "        \n",
    "        t = t + 1\n",
    "        learning_rate = learning_rate0/(1+decay_rate*i)\n",
    "        \n",
    "        parameters, v, s = Adam_optimizer(parameters, grads, v, s,\n",
    "                                                               t,m, learning_rate, beta1, beta2,  epsilon,)\n",
    "        if i % print_after == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if  i % print_after == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per '+str(print_after)+')')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Y,Yhat,Set):\n",
    "    spos=0\n",
    "    \n",
    "    for i in range(Y.shape[1]): \n",
    "        if Y[0,i]==1 and Yhat[0,i]==1:\n",
    "            spos = spos+1\n",
    "            \n",
    "    p = spos /np.sum(Yhat == 1)\n",
    "    r = spos/ np.sum( Y == 1)\n",
    "    acc = np.mean(Y == Yhat)\n",
    "    f1score = 2*p*r/(p+r)\n",
    "    \n",
    "    #print(Set+\" :       \"+str(p) + \"  \"+str(r)+\"  \"+str(f1score)+\"  \"+str(acc))\n",
    "    data = [{'Precision': p, 'Recall': r, 'Accuracy': acc,'F-score': f1score}] \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    display(df)\n",
    "    error = (1-acc)*100\n",
    "    print(Set+\" :       \"+'%0.3f'%error+\" %\" +'\\t'+str(f1score))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.717497\n",
      "Cost after iteration 50: 0.675875\n",
      "Cost after iteration 100: 0.627631\n",
      "Cost after iteration 150: 0.568555\n",
      "Cost after iteration 200: 0.514498\n",
      "Cost after iteration 250: 0.462803\n",
      "Cost after iteration 300: 0.415151\n",
      "Cost after iteration 350: 0.372427\n",
      "Cost after iteration 400: 0.335095\n",
      "Cost after iteration 450: 0.303747\n",
      "Cost after iteration 500: 0.278130\n",
      "Cost after iteration 550: 0.258924\n",
      "Cost after iteration 600: 0.245710\n",
      "Cost after iteration 650: 0.236877\n",
      "Cost after iteration 700: 0.230869\n",
      "Cost after iteration 750: 0.226560\n",
      "Cost after iteration 800: 0.223821\n",
      "Cost after iteration 850: 0.222727\n",
      "Cost after iteration 900: 0.221568\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.931507</td>\n",
       "      <td>0.924658</td>\n",
       "      <td>0.92517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision    Recall  Accuracy  F-score\n",
       "0   0.918919  0.931507  0.924658  0.92517"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  :       7.534 %\t0.9251700680272108\n",
      "Seed of initialization : 3\n"
     ]
    }
   ],
   "source": [
    "global dropout_cache\n",
    "global keep_prob\n",
    "global lambd\n",
    "\n",
    "np.random.seed(3)\n",
    "keep_prob=1\n",
    "lambd=0\n",
    "decay_rate=0\n",
    "\n",
    "p = model(train_X, train_Y, layers_dims = [10,64,32,16,8,4,1], epocs =901, \n",
    "                           learning_rate0 = 0.000088,  beta1 = 0.9, beta2 = 0.9,  epsilon = 1e-8, \n",
    "                           print_after = 50)\n",
    "\n",
    "def predict(X,p):\n",
    "    keep_prob=1\n",
    "    AL = forwardprop(X, p)[0]\n",
    "    Y_prediction = AL\n",
    "    for i in range(AL.shape[1]):\n",
    "          Y_prediction[0, i] = 1 if AL[0, i] > 0.5 else 0\n",
    "   \n",
    "    return Y_prediction \n",
    "\n",
    "test_Yhat = predict(test_X,p)\n",
    "train_Yhat = predict(train_X,p)\n",
    "\n",
    "evaluate(test_Y,test_Yhat,\"Test \")\n",
    "\n",
    "print(\"Seed of initialization : \"+str(3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
