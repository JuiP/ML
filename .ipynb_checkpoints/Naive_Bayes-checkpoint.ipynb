{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# to break into sentences from text\n",
    "def sentence(text):\n",
    "    sentences = []\n",
    "    sentences = list(text.split(\"\\n\"))\n",
    "    return sentences\n",
    "    \n",
    "\n",
    "# load the document\n",
    "filename = 'a1_data/a1_d3.txt'\n",
    "text = load_doc(filename)\n",
    "sentences = sentence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing\n",
    "lower_case_sentences = []\n",
    "for i in sentences:\n",
    "    lower_case_sentences.append(i.lower())\n",
    "\n",
    "no_punctuations = []\n",
    "for i in lower_case_sentences:\n",
    "    no_punctuations.append(''.join(c for c in i if c not in string.punctuation))\n",
    "\n",
    "clean_data = []\n",
    "for i in no_punctuations:\n",
    "    sub = i.split(', ')\n",
    "    sub1 = sub[0].split('\\t')\n",
    "    clean_data.append(sub1)\n",
    "clean_data.remove(clean_data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review Sentiment\n",
      "0    so there is no way for me to plug it in here i...         0\n",
      "1                            good case excellent value         1\n",
      "2                                great for the jawbone         1\n",
      "3    tied to charger for conversations lasting more...         0\n",
      "4                                     the mic is great         1\n",
      "..                                                 ...       ...\n",
      "995  the screen does get smudged easily because it ...         0\n",
      "996  what a piece of junk i lose more calls on this...         0\n",
      "997                        item does not match picture         0\n",
      "998  the only thing that disappoint me is the infra...         0\n",
      "999  you can not answer calls with the unit never w...         0\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(clean_data, columns =['Review', 'Sentiment'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into 5 folds\n",
    "def cross_validation(df, n_folds):\n",
    "    df_split = list()\n",
    "    df_copy = list(df)\n",
    "    fold_size = int(len(df) / 5)\n",
    "    for i in range (n_folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            index = random.randrange(0,len(df_copy))\n",
    "            fold.append(df_copy.pop(index))\n",
    "        df_split.append(fold)\n",
    "    return df_split\n",
    "\n",
    "folds = cross_validation(clean_data, 5)\n",
    "for fold in folds:\n",
    "    train_set = list(folds)\n",
    "    train_set.remove(fold)\n",
    "    train_set = sum(train_set, [])\n",
    "    test_set = list()\n",
    "    for row in fold:\n",
    "        row_copy = list(row)\n",
    "        test_set.append(row_copy)\n",
    "        \n",
    "train_df = pd.DataFrame(train_set, columns =['Review', 'Sentiment'])\n",
    "test_df = pd.DataFrame(test_set,columns =['Review', 'Sentiment'])\n",
    "train_df_positive = train_df.loc[train_df['Sentiment']=='1']\n",
    "train_df_negative = train_df.loc[train_df['Sentiment']=='0']\n",
    "\n",
    "# Setting the model's vocabulary\n",
    "def vocab_freq(train_df):\n",
    "    train_sentences = train_df['Review'].values\n",
    "    train_sentences_list = train_sentences.tolist()\n",
    "    all_words_train = []\n",
    "    for i in train_sentences_list:\n",
    "        all_words_train.extend(i.split(' '))\n",
    "    vocab,count = np.unique(np.array(all_words_train),return_counts=True)\n",
    "    return (vocab,count)\n",
    "\n",
    "#Setting the positive sentiment and negative sentiment vocab and frequency\n",
    "vocab_positive, count_positive = vocab_freq(train_df_positive)\n",
    "vocab_negative, count_negative = vocab_freq(train_df_negative)\n",
    "vocab_total, count_total = vocab_freq(train_df)\n",
    "\n",
    "\n",
    "#Gives the probability P(C) or prior probability\n",
    "# no. of sentiment values is the same as the no. of reviews in train_set\n",
    "train_sentiments = train_df['Sentiment'].values\n",
    "sentiment,count = np.unique(train_sentiments,return_counts=True)\n",
    "\n",
    "positive_review_count = count[1]\n",
    "negative_review_count = count[0]\n",
    "\n",
    "prob_positive = positive_review_count / (positive_review_count + negative_review_count)\n",
    "prob_negative = negative_review_count / (positive_review_count + negative_review_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method index of list object at 0x7f58936de820>\n",
      "[2.4395721745304103e-09, 2.4295171532861695e-41, 5.74837690927997e-13, 2.6138273298540106e-10, 2.3530906773550855e-16, 1.4041638861405757e-13, 0.0002317497103128621, 3.8315942210391855e-49, 4.1200920593660167e-38, 2.283915237741065e-40, 7.58650293946324e-37, 6.916534345166263e-16, 3.373787529145074e-32, 2.5898543829638684e-11, 1.3776319557691678e-33, 5.269733697856123e-13, 1.8973975542985289e-10, 6.454434791532202e-16, 6.665259691127728e-08, 1.386636081037784e-15, 5.0177124431447315e-42, 1.726569588642579e-12, 1.5360467473807316e-05, 1.826069559823247e-05, 2.7186123963605865e-35, 1.3199828015762753e-08, 0.00220162224797219, 3.187020665476371e-12, 9.581680823249732e-49, 2.888279199488682e-07, 4.895073185980047e-46, 4.5920278636731656e-05, 2.1912834717878967e-05, 5.754184188593717e-14, 1.350194873972976e-10, 5.478208679469742e-06, 5.084278483477002e-41, 1.996470730195888e-33, 0.0008111239860950173, 2.041274676647894e-09, 9.991478610734556e-37, 2.0818885337670073e-47, 7.049196498879825e-09, 3.3030375861508737e-06, 4.166483875781884e-49, 1.0927706349834812e-63, 1.630726302050496e-51, 1.2980436996460908e-12, 4.337327969324209e-19, 4.213241362819452e-20, 2.449081527292355e-05, 2.0818014167237748e-20, 2.04640583320996e-28, 1.160091249770063e-05, 1.4069939371809733e-09, 1.2653138884607856e-45, 6.77105746400277e-08, 4.0343911539038195e-27, 3.313044715048809e-15, 9.371716039877481e-30, 7.79253922771926e-15, 1.5660351840653303e-28, 8.950270917799722e-09, 0.008342989571263035, 2.0228778446951855e-26, 2.0906327286965043e-29, 6.601781027402701e-07, 1.954819197168355e-09, 6.345991420491997e-12, 5.3396758309874795e-08, 3.2710701231081477e-12, 2.034404060902125e-28, 2.1009728450338718e-33, 1.9914874894125797e-09, 9.744517087136116e-16, 9.458782770964528e-12, 2.500104059110946e-05, 2.0422554207765156e-12, 1.7429726607714706e-14, 3.207476510569287e-12, 6.756457371346014e-05, 2.845523080543986e-39, 1.135177998001942e-29, 1.839333293413276e-17, 4.4969622465583903e-35, 7.321526747077687e-24, 6.158900055249674e-10, 9.708501510886325e-09, 1.6796024527363808e-16, 4.7457797598345586e-24, 6.489404185521556e-18, 1.1104037059357673e-21, 1.8539367452961296e-33, 2.087971473721781e-32, 3.2223559754479925e-30, 8.030217543490574e-10, 2.5000973163893207e-21, 7.998401353596407e-14, 0.00013147700830727378, 3.398419570733991e-26, 1.5199448330908741e-27, 2.0537214734567224e-08, 4.185857766809066e-08, 2.118238317228385e-30, 5.761328606169624e-14, 1.8776099789881063e-32, 3.847949588646417e-09, 1.3291825271359701e-24, 2.846010113199954e-42, 3.193965878778471e-34, 5.6284415169523016e-08, 6.9055924821172e-11, 2.734826440302886e-19, 2.1087983614985876e-45, 3.7718327216275406e-32, 1.7835534284219003e-17, 9.193332713127285e-31, 1.0246493149270292e-43, 2.7258917692567905e-12, 8.210173137981893e-26, 5.770090507382525e-19, 0.0006952491309385864, 2.1806507956139547e-05, 8.362737803687108e-30, 2.1960110525087275e-43, 3.92266627465908e-30, 3.248330043845206e-26, 3.880285299475379e-11, 3.264752876829453e-12, 1.2993867675591188e-20, 2.0252946662976737e-24, 9.977564827666915e-43, 8.722640205546672e-09, 8.227431811238045e-11, 3.399195440061724e-16, 2.0403790453700284e-37, 9.787767182040594e-17, 4.886614212747132e-23, 1.0770219720578468e-13, 2.3059176485768832e-32, 3.3940495992138796e-18, 1.1412917619840775e-11, 2.8660804888185677e-11, 4.566118567876845e-17, 3.549973875203848e-39, 1.2974912728747633e-21, 3.0494652181630122e-09, 1.4538392097240117e-49, 0.00013147700830727378, 1.0735102203694002e-33, 1.5078940977555417e-10, 7.778928026295098e-17, 1.0057875798704504e-17, 6.661539873666552e-29, 5.606771468209513e-28, 1.8261773323892716e-15, 2.1816433415369402e-45, 0.00660486674391657, 1.1314138299225217e-08, 1.7063685392214366e-17, 8.317183917764831e-42, 2.2767273491485204e-37, 2.972871841909222e-24, 0.5, 1.692075275043208e-20, 4.451410629357694e-18, 1.0741585646019101e-06, 3.485103106472014e-09, 2.4457732130213457e-15, 0.0035921205098493627, 4.610673399668497e-19, 1.9985444150884221e-19, 8.094280693461089e-32, 0.009849362688296639, 0.0002317497103128621, 0.0002932452881363214, 3.721656857273453e-55, 1.2408423034401315e-13, 8.254849370044056e-23, 2.59902908965161e-27, 1.7722623312501287e-11, 3.2224756938057305e-07, 7.973312109322127e-24, 2.0544137634298675e-09, 8.224061561297263e-13, 1.7344407550393982e-15, 8.944386453027994e-10, 3.35137472155796e-05, 3.9274217354735447e-25, 4.0739966191549903e-35, 3.272935214866619e-13, 6.391243459381366e-06, 6.597548392273745e-27, 2.5194036791045718e-17, 3.2439588650977686e-05, 8.120808644593613e-17, 6.444951387611461e-07, 1.1202117127945762e-08, 6.161164420370168e-10, 8.241019840313099e-10]\n"
     ]
    }
   ],
   "source": [
    "# extracting the words from the test_set\n",
    "test_sentences = test_df['Review'].values\n",
    "test_sentences_list = test_sentences.tolist()\n",
    "words_test = []\n",
    "for i in test_sentences_list:\n",
    "    words_test.append(i.split(' '))\n",
    "print (list(words_test).index)\n",
    "\n",
    "#Calculating likelihood probability P(d|C)\n",
    "#Writing a function for a given class C = 1,0\n",
    "def posterior_prob(train_df,vocab,count,words_test,prob_class, class_count):\n",
    "    posterior_prob = list()\n",
    "    #Calculations for test data in row i\n",
    "    for i in words_test:\n",
    "        likelihood_prob = 1\n",
    "        word_test_array = np.array(i)\n",
    "        vocab_test,count_test = np.unique(word_test_array,return_counts=True)\n",
    "        #j returns the elements of the iterable list i\n",
    "        for j in i:\n",
    "            try:\n",
    "                index = list(vocab).index(j)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            index1 = list(vocab_test).index(j)\n",
    "            #here likelihood probability is returned for the ith row of test data\n",
    "            likelihood_prob *= count_test[index1]*((count[index]+1)/(np.sum(count)+class_count))\n",
    "        #return the probability P(d|C)*P(C)\n",
    "        posterior = prob_class*likelihood_prob\n",
    "        posterior_prob.append(posterior)\n",
    "    return posterior_prob\n",
    "            \n",
    "posterior_prob_positive = posterior_prob(train_df,vocab_positive,count_positive,words_test,prob_positive,positive_review_count)\n",
    "posterior_prob_negative = posterior_prob(train_df,vocab_negative,count_negative,words_test,prob_negative,negative_review_count)\n",
    "print (posterior_prob_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing f1 score\n",
    "def f1_score_single(y_actual, y_pred):\n",
    "    y_actual = set(y_actual)\n",
    "    y_pred = set(y_pred)\n",
    "    true_positives = len(y_actual & y_pred)\n",
    "    if true_positives == 0: return 0.\n",
    "    precision = 1.0* true_positives / len(y_pred)\n",
    "    recall = 1.0* true_positives / len(y_actual)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "def f1_score(y_actual, y_pred):\n",
    "    return np.mean([f1_score_single(x, y) for x, y in zip(y_actual, y_pred)])\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review Sentiment  \\\n",
      "0                           having trouble with volume         0   \n",
      "1    this is cool because most cases are just open ...         1   \n",
      "2             the construction of the headsets is poor         0   \n",
      "3                                     doesnt last long         0   \n",
      "4                           will order from them again         1   \n",
      "..                                                 ...       ...   \n",
      "195          we have gotten a lot of compliments on it         1   \n",
      "196                                       fast service         1   \n",
      "197                 excellent starter wireless headset         0   \n",
      "198                 plantronics bluetooth excelent buy         1   \n",
      "199                           good product good seller         1   \n",
      "\n",
      "    Predicted Sentiment  \n",
      "0                     1  \n",
      "1                     0  \n",
      "2                     1  \n",
      "3                     0  \n",
      "4                     0  \n",
      "..                  ...  \n",
      "195                   1  \n",
      "196                   0  \n",
      "197                   0  \n",
      "198                   1  \n",
      "199                   0  \n",
      "\n",
      "[200 rows x 3 columns]\n",
      "0.44\n",
      "44.0\n"
     ]
    }
   ],
   "source": [
    "test_predict = list()\n",
    "\n",
    "#predict the Sentiment\n",
    "for i in range (len(test_set)):\n",
    "    if posterior_prob_positive[i] > posterior_prob_negative[i]:\n",
    "        test_predict.append(\"1\")\n",
    "    else:\n",
    "        test_predict.append(\"0\")\n",
    "        \n",
    "test_df['Predicted Sentiment'] = test_predict \n",
    "f1_score = f1_score(test_df['Sentiment'], test_df['Predicted Sentiment'])\n",
    "accuracy = accuracy_metric(test_df['Sentiment'], test_df['Predicted Sentiment'])\n",
    "print(test_df)\n",
    "print(f1_score)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
